#!/usr/bin/env python
"""
æ™ºè°± Batch API Embedding åŒ…è£…å™¨

æä¾›ä¸ LightRAG å…¼å®¹çš„ embedding å‡½æ•°ï¼Œæ”¯æŒï¼š
1. å®æ—¶ API æ¨¡å¼ï¼ˆå°æ‰¹é‡ï¼‰
2. Batch API æ¨¡å¼ï¼ˆå¤§æ‰¹é‡ï¼Œæ— å¹¶å‘é™åˆ¶ï¼Œ50% æˆæœ¬èŠ‚çœï¼‰

ä½¿ç”¨æ–¹å¼ï¼š
    from src.tools.zhipu_batch_embedding_wrapper import batch_embed_func

    # ä½¿ç”¨ Batch API
    embedding_func = batch_embed_func(
        embedding_dim=2048,
        api_key="your-api-key",
        model="embedding-3",
        use_batch=True,  # å¯ç”¨ Batch æ¨¡å¼
        batch_threshold=100,  # è¶…è¿‡ 100 æ¡æ–‡æœ¬æ—¶ä½¿ç”¨ Batch
    )

    vectors = embedding_func(["æ–‡æœ¬1", "æ–‡æœ¬2", ...])
"""

import os
from typing import Any

from lightrag.utils import EmbeddingFunc
from openai import OpenAI

from src.core.core import get_embedding_config


class BatchEmbeddingWrapper:
    """
    æ™ºè°± Batch Embedding åŒ…è£…å™¨

    æä¾›ä¸ LightRAG EmbeddingFunc å…¼å®¹çš„æ¥å£ï¼Œè‡ªåŠ¨é€‰æ‹©ï¼š
    - å®æ—¶ APIï¼šæ–‡æœ¬æ•°é‡ < batch_threshold
    - Batch APIï¼šæ–‡æœ¬æ•°é‡ >= batch_threshold
    """

    def __init__(
        self,
        embedding_dim: int,
        api_key: str,
        base_url: str,
        model: str,
        max_token_size: int = 8192,
        use_batch: bool = True,
        batch_threshold: int = 100,
    ):
        """
        åˆå§‹åŒ– Batch Embedding åŒ…è£…å™¨

        Args:
            embedding_dim: Embedding å‘é‡ç»´åº¦
            api_key: æ™ºè°± AI API Key
            base_url: API åŸºç¡€ URL
            model: Embedding æ¨¡å‹åç§°
            max_token_size: æœ€å¤§ token æ•°
            use_batch: æ˜¯å¦å¯ç”¨ Batch API
            batch_threshold: Batch API è§¦å‘é˜ˆå€¼
        """
        self.embedding_dim = embedding_dim
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.max_token_size = max_token_size
        self.use_batch = use_batch
        self.batch_threshold = batch_threshold

        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆç”¨äºå®æ—¶ APIï¼‰
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url,
        )

        # å»¶è¿Ÿå¯¼å…¥ Batch API å®¢æˆ·ç«¯ï¼ˆä»…åœ¨éœ€è¦æ—¶ï¼‰
        self._batch_client = None

    @property
    def batch_client(self):
        """å»¶è¿ŸåŠ è½½ Batch API å®¢æˆ·ç«¯"""
        if self._batch_client is None:
            try:
                from src.tools.zhipu_batch_embedding import BatchEmbeddingClient

                self._batch_client = BatchEmbeddingClient(
                    api_key=self.api_key,
                    batch_threshold=self.batch_threshold,
                )
            except ImportError as e:
                print(f"âš ï¸ æ— æ³•å¯¼å…¥ Batch API å®¢æˆ·ç«¯: {e}")
                print("å°†ä½¿ç”¨å®æ—¶ API ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")
                self._batch_client = False  # æ ‡è®°ä¸ºä¸å¯ç”¨
        return self._batch_client

    def __call__(self, texts: list[str]) -> list[list[float]]:
        """
        å¯¹æ–‡æœ¬åˆ—è¡¨è¿›è¡Œ Embedding

        Args:
            texts: å¾… Embedding çš„æ–‡æœ¬åˆ—è¡¨

        Returns:
            å‘é‡åˆ—è¡¨
        """
        # ç©ºåˆ—è¡¨å¤„ç†
        if not texts:
            return []

        # å†³å®šä½¿ç”¨å“ªç§ API
        use_batch_api = (
            self.use_batch
            and len(texts) >= self.batch_threshold
            and self.batch_client is not False
        )

        if use_batch_api:
            return self._embed_batch(texts)
        else:
            return self._embed_realtime(texts)

    def _embed_realtime(self, texts: list[str]) -> list[list[float]]:
        """ä½¿ç”¨å®æ—¶ API è¿›è¡Œ Embedding"""
        embeddings = []

        # æ™ºè°± API æ”¯æŒæ‰¹é‡è¯·æ±‚ï¼ˆæœ€å¤š 64 æ¡ï¼‰
        batch_size = 64

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]

            try:
                response = self.client.embeddings.create(
                    model=self.model,
                    input=batch,
                )

                # æå–å‘é‡
                batch_embeddings = [item.embedding for item in response.data]
                embeddings.extend(batch_embeddings)

            except Exception as e:
                print(f"âš ï¸ å®æ—¶ API Embedding å¤±è´¥: {e}")
                # è¿”å›é›¶å‘é‡ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
                embeddings.extend([[0.0] * self.embedding_dim] * len(batch))

        return embeddings

    def _embed_batch(self, texts: list[str]) -> list[list[float]]:
        """ä½¿ç”¨ Batch API è¿›è¡Œ Embedding"""
        if self.batch_client is False:
            # Batch å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œå›é€€åˆ°å®æ—¶ API
            print("âš ï¸ Batch API ä¸å¯ç”¨ï¼Œä½¿ç”¨å®æ—¶ API")
            return self._embed_realtime(texts)

        try:
            print(f"ğŸ“¦ ä½¿ç”¨ Batch API å¤„ç† {len(texts)} æ¡æ–‡æœ¬...")
            embeddings = self.batch_client.embed_texts(
                texts=texts,
                model=self.model,
            )
            return embeddings
        except Exception as e:
            print(f"âš ï¸ Batch API å¤±è´¥: {e}ï¼Œå›é€€åˆ°å®æ—¶ API")
            return self._embed_realtime(texts)


def batch_embed_func(
    embedding_dim: int = 2048,
    api_key: str | None = None,
    base_url: str | None = None,
    model: str | None = None,
    max_token_size: int = 8192,
    use_batch: bool = True,
    batch_threshold: int = 100,
) -> EmbeddingFunc:
    """
    åˆ›å»ºæ”¯æŒ Batch API çš„ Embedding å‡½æ•°

    Args:
        embedding_dim: Embedding å‘é‡ç»´åº¦
        api_key: æ™ºè°± AI API Keyï¼ˆå¦‚æœä¸º Noneï¼Œä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        base_url: API åŸºç¡€ URLï¼ˆå¦‚æœä¸º Noneï¼Œä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        model: Embedding æ¨¡å‹åç§°ï¼ˆå¦‚æœä¸º Noneï¼Œä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        max_token_size: æœ€å¤§ token æ•°
        use_batch: æ˜¯å¦å¯ç”¨ Batch API
        batch_threshold: Batch API è§¦å‘é˜ˆå€¼

    Returns:
        LightRAG å…¼å®¹çš„ EmbeddingFunc å¯¹è±¡

    Example:
        >>> from src.tools.zhipu_batch_embedding_wrapper import batch_embed_func
        >>>
        >>> # ä½¿ç”¨ Batch æ¨¡å¼
        >>> embedding_func = batch_embed_func(
        ...     embedding_dim=2048,
        ...     use_batch=True,
        ...     batch_threshold=100,
        ... )
        >>>
        >>> vectors = embedding_func(["æ–‡æœ¬1", "æ–‡æœ¬2"])
    """
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼ˆå¦‚æœæœªæä¾›ï¼‰
    if api_key is None or base_url is None or model is None:
        config = get_embedding_config()
        if api_key is None:
            api_key = config["api_key"]
        if base_url is None:
            base_url = config["base_url"]
        if model is None:
            model = config["model"]
        if embedding_dim == 2048 and "dim" in config:
            embedding_dim = config["dim"]

    # åˆ›å»ºåŒ…è£…å™¨å®ä¾‹
    wrapper = BatchEmbeddingWrapper(
        embedding_dim=embedding_dim,
        api_key=api_key,
        base_url=base_url,
        model=model,
        max_token_size=max_token_size,
        use_batch=use_batch,
        batch_threshold=batch_threshold,
    )

    # åˆ›å»º LightRAG å…¼å®¹çš„ EmbeddingFunc
    return EmbeddingFunc(
        embedding_dim=embedding_dim,
        max_token_size=max_token_size,
        func=wrapper,
    )


def create_embedding_func_from_config(
    config: dict[str, Any],
    use_batch: bool = True,
    batch_threshold: int = 100,
) -> EmbeddingFunc:
    """
    ä»é…ç½®å­—å…¸åˆ›å»º Embedding å‡½æ•°

    Args:
        config: é…ç½®å­—å…¸ï¼ˆä» get_embedding_config() è·å–ï¼‰
        use_batch: æ˜¯å¦å¯ç”¨ Batch API
        batch_threshold: Batch API è§¦å‘é˜ˆå€¼

    Returns:
        LightRAG å…¼å®¹çš„ EmbeddingFunc å¯¹è±¡
    """
    return batch_embed_func(
        embedding_dim=config.get("dim", 2048),
        api_key=config.get("api_key"),
        base_url=config.get("base_url"),
        model=config.get("model"),
        max_token_size=config.get("max_tokens", 8192),
        use_batch=use_batch,
        batch_threshold=batch_threshold,
    )


__all__ = [
    "batch_embed_func",
    "create_embedding_func_from_config",
    "BatchEmbeddingWrapper",
]
